{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping - Selenium + BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* Author:  [Yuttapong Mahasittiwat](mailto:khala1391@gmail.com)\n",
    "* Technologist | Data Modeler | Data Analyst\n",
    "* [YouTube](https://www.youtube.com/khala1391)\n",
    "* [LinkedIn](https://www.linkedin.com/in/yuttapong-m/)\n",
    "* [Tableau](https://public.tableau.com/app/profile/yuttapong.m/vizzes)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: [WS CubeTech youtube channel](https://www.youtube.com/watch?v=UabBGhnVqSo&list=PLc20sA5NNOvrsn3a78ewy2VTCXVV47NB4&index=1&t=0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-21 22:49:38.655263\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------------------------------------\n",
    "## import library\n",
    "\n",
    "from urllib.request import urlopen  # option#1\n",
    "import requests  # option#2\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Define the dynamic XPath for the sub-menu items\n",
    "xpaths = [f'//*[@id=\"mainmenu1\"]/li[2]/ul/li[2]/ul/li[{i}]/a' for i in range(4,15)]\n",
    "\n",
    "# Loop through each XPath item\n",
    "for path2 in xpaths:\n",
    "    try:\n",
    "        ## --------------------------------------------------------------------------\n",
    "        ## setup selenium\n",
    "\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(\"https://www.naiin.com/\")\n",
    "\n",
    "        # Navigate to the website\n",
    "        driver.maximize_window()\n",
    "\n",
    "        # Locate the element to perform the hover action on\n",
    "        path = '//*[@id=\"mainmenu1\"]/li[2]/a'\n",
    "        element_to_hover = driver.find_element(By.XPATH, path)\n",
    "\n",
    "        # Create an instance of the ActionChains class\n",
    "        actions = ActionChains(driver)\n",
    "\n",
    "        # Perform the hover action\n",
    "        actions.move_to_element(element_to_hover).perform()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Locate the element for each sub-category\n",
    "        element_to_hover2 = driver.find_element(By.XPATH, path2)\n",
    "\n",
    "        # Perform hover action on the submenu and click\n",
    "        actions.move_to_element(element_to_hover2).perform()\n",
    "        time.sleep(3)\n",
    "        actions.click(element_to_hover2).perform()\n",
    "        time.sleep(3)\n",
    "\n",
    "        ## --------------------------------------------------------------------------\n",
    "        ## setup BeautifulSoup to scrape data\n",
    "\n",
    "        url = driver.current_url\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "        # Scrape books, authors, and prices\n",
    "        books = soup.find_all(\"a\", class_=\"itemname\")\n",
    "        book_list = [book.text.strip() for book in books]\n",
    "        print(f\"Books found: {len(book_list)}\")\n",
    "\n",
    "        authors = soup.find_all(\"a\", class_=\"inline-block tw-whitespace-normal tw-block\")\n",
    "        author_list = [author.text.strip() for author in authors]\n",
    "        print(f\"Authors found: {len(author_list)}\")\n",
    "\n",
    "        prices = soup.find_all(\"p\", class_=\"txt-price\")\n",
    "        price_list = [price.text.strip() for price in prices]\n",
    "        print(f\"Prices found: {len(price_list)}\")\n",
    "\n",
    "        ## --------------------------------------------------------------------------\n",
    "        ## while loop to handle pagination\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                np = soup.find(\"a\", class_=\"nav-pag pag-next\").get(\"href\")\n",
    "                print(f\"Next page URL: {np}\")\n",
    "\n",
    "                url = np\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text, \"lxml\")\n",
    "                time.sleep(2)\n",
    "\n",
    "                books = soup.find_all(\"a\", class_=\"itemname\")\n",
    "                book_list.extend([book.text.strip() for book in books])\n",
    "                print(f\"Books total: {len(book_list)}\")\n",
    "\n",
    "                authors = soup.find_all(\"a\", class_=\"inline-block tw-whitespace-normal tw-block\")\n",
    "                author_list.extend([author.text.strip() for author in authors])\n",
    "                print(f\"Authors total: {len(author_list)}\")\n",
    "\n",
    "                prices = soup.find_all(\"p\", class_=\"txt-price\")\n",
    "                price_list.extend([price.text.strip() for price in prices])\n",
    "                print(f\"Prices total: {len(price_list)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(f\"Total books: {len(book_list)}\")\n",
    "                print(f\"Total authors: {len(author_list)}\")\n",
    "                print(f\"Total prices: {len(price_list)}\")\n",
    "                break\n",
    "\n",
    "        ## --------------------------------------------------------------------------\n",
    "        ## save each category's data as CSV\n",
    "\n",
    "        df = pd.DataFrame({\"book\": book_list,\n",
    "                           \"author\": author_list,\n",
    "                           \"price\": price_list})\n",
    "        df.to_csv(f'data/book_details_category_{xpaths.index(path2)+1}.csv', encoding='utf-8-sig')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing path {path2}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
